[
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "Lab: Futureverse 2",
    "section": "",
    "text": "Note\n\n\n\nThis is the second of two parallelization labs. It will take you through Futureverse functions that you and others are likely to use to parallelize R code. We will cover the future.apply package, furrr package, and foreach with the doFuture package. In your R career, you can pick whichever you prefer - they are all equally good.\nYou are highly encouraged to test things out yourself and tweak things to figure out how these methods behave.\nSlides: You’ll find the slides in the menus above."
  },
  {
    "objectID": "lab2.html#setup",
    "href": "lab2.html#setup",
    "title": "Lab: Futureverse 2",
    "section": "Setup",
    "text": "Setup\nIt is assumed that you have already installed common Futureverse packages in Lab 1. In this second part, we will use a not-so-slow-but-still-slow version of slow_sum();\n\nlibrary(future)\nlibrary(progressr)\n\nslow_sum &lt;- function(x) {\n  sum &lt;- 0\n  for (value in x) {\n    Sys.sleep(0.1)     ## 0.1 second slowdown per value\n    sum &lt;- sum + value\n  }\n  sum\n}"
  },
  {
    "objectID": "lab2.html#exercises",
    "href": "lab2.html#exercises",
    "title": "Lab: Futureverse 2",
    "section": "Exercises",
    "text": "Exercises\n\nRecap from Lab 1\nIn the first part - Lab 1 - we learned about the future() and value() functions part of the future package. They allow us to run independent tasks like:\n\nxs &lt;- list(1:25, 26:50, 51:75, 76:100)\na &lt;- slow_sum(xs[[1]])\nb &lt;- slow_sum(xs[[2]])\nc &lt;- slow_sum(xs[[3]])\nd &lt;- slow_sum(xs[[4]])\ny &lt;- a + b + c + d\ny\n\n[1] 5050\n\n\nin parallel. We learned how to do:\n\nlibrary(future)\nplan(multisession, workers = 4)\n\nxs &lt;- list(1:25, 26:50, 51:75, 76:100)\nfa &lt;- future(slow_sum(xs[[1]]))\nfb &lt;- future(slow_sum(xs[[2]]))\nfc &lt;- future(slow_sum(xs[[3]]))\nfd &lt;- future(slow_sum(xs[[4]]))\ny &lt;- value(fa) + value(fb) + value(fc) + value(fd)\ny\n\n[1] 5050\n\n\nWe then learned how to generalize this to a for-loop, by realizing we can do:\n\nlibrary(future)\nplan(multisession, workers = 4)\n\nxs &lt;- list(1:25, 26:50, 51:75, 76:100)\n\nfs &lt;- list()\nfs[[1]] &lt;- future(slow_sum(xs[[1]]))\nfs[[2]] &lt;- future(slow_sum(xs[[2]]))\nfs[[3]] &lt;- future(slow_sum(xs[[3]]))\nfs[[4]] &lt;- future(slow_sum(xs[[4]]))\n\nys &lt;- list()\nys[[1]] &lt;- value(fs[[1]])\nys[[2]] &lt;- value(fs[[2]])\nys[[3]] &lt;- value(fs[[3]])\nys[[4]] &lt;- value(fs[[4]])\n\nys &lt;- unlist(ys)\ny &lt;- sum(ys)\ny\n\n[1] 5050\n\n\nand then simplify as:\n\nlibrary(future)\nplan(multisession, workers = 4)\n\nxs &lt;- list(1:25, 26:50, 51:75, 76:100)\n\nfs &lt;- list()\nfor (ii in seq_along(xs)) {\n  fs[[ii]] &lt;- future(slow_sum(xs[[ii]]))\n}\n\nys &lt;- list()\nfor (ii in seq_along(fs)) {\n  ys[[ii]] &lt;- value(fs[[ii]])\n}\n\nys &lt;- unlist(ys)\ny &lt;- sum(ys)\ny\n\n[1] 5050\n\n\nWe then got rid of the for-loops in the auxillary index ii, by using lapply():\n\nlibrary(future)\nplan(multisession, workers = 4)\n\nxs &lt;- list(1:25, 26:50, 51:75, 76:100)\n\nfs &lt;- lapply(xs, function(x) { future(slow_sum(x)) })\nys &lt;- lapply(fs, value)\n\nys &lt;- unlist(ys)\ny &lt;- sum(ys)\ny\n\n[1] 5050\n\n\nFinally, we turned this into a utility function:\n\nparallel_lapply &lt;- function(X, FUN) {\n  fs &lt;- lapply(X, function(x) {\n    future(FUN(x))\n  })\n  lapply(fs, value)\n}\n\nsuch that we can do:\n\nlibrary(future)\nplan(multisession, workers = 4)\n\nxs &lt;- list(1:25, 26:50, 51:75, 76:100)\nys &lt;- parallel_lapply(xs, slow_sum)\nys &lt;- unlist(ys)\ny &lt;- sum(ys)\ny\n\n[1] 5050\n\n\n\n\nParallel versions of purrr::map()\nTask 1:\nWrite a parallel_map() function that emulates what the map() function of the purrr package does, while at the same time running in parallel using futures. We want to create a parallel version of:\n\nlibrary(purrr)\nxs &lt;- list(1:25, 26:50, 51:75, 76:100)\nys &lt;- map(xs, slow_sum)\nys &lt;- unlist(ys)\ny &lt;- sum(ys)\ny\n\n[1] 5050\n\n\nWe want to use the same argument names as map();\n\nargs(map)\n\nfunction (.x, .f, ..., .progress = FALSE) \nNULL\n\n\nso that users of our parallel_map() will feel at home. For simplicity, you can ignore arguments ... and .progress. So, let’s create a function:\n\nparallel_map &lt;- function(.x, .f) {\n  ## something here\n}\n\nI recommend that you modify the existing parallel_lapply(). Verify that it works with:\n\nlibrary(future)\nplan(multisession, workers = 4)\n\nxs &lt;- list(1:25, 26:50, 51:75, 76:100)\nys &lt;- parallel_map(xs, slow_sum)\nys &lt;- unlist(ys)\ny &lt;- sum(ys)\ny\n\n\n\nSolution\n\n\nlibrary(purrr)\n\nparallel_map &lt;- function(.x, .f) {\n  fs &lt;- map(.x, function(x) {\n    future(.f(x))\n  })\n  map(fs, value)\n}\n\n\nTask 2:\nJust like lapply() and map() return list, parallel_lapply() and parallel_map() return lists. But, as in our example, it’s common that one wants the atomic vector version of it, which is why we do:\n\nys &lt;- unlist(ys)\nys\n\n[1]  325  950 1575 2200\n\n\nHaving to call this each time is tedious and adds friction and noise to our code. When not parallelizing, we can use purrr’s map_dbl() to achieve the same in a one go;\n\nlibrary(purrr)\nxs &lt;- list(1:25, 26:50, 51:75, 76:100)\nys &lt;- map_dbl(xs, slow_sum)\ny &lt;- sum(ys)\ny\n\n[1] 5050\n\n\nWrite your own parallel_map_dbl() that achieves the same, but via futures, so that you can run:\n\nlibrary(purrr)\nxs &lt;- list(1:25, 26:50, 51:75, 76:100)\nys &lt;- parallel_map_dbl(xs, slow_sum)\ny &lt;- sum(ys)\ny\n\nHint: Don’t use unlist() - instead make use of map_dbl(). But think carefully where in your function you want to use map_dbl().\n\n\nSolution\n\n\nlibrary(purrr)\n\nparallel_map_dbl &lt;- function(.x, .f) {\n  fs &lt;- map(.x, function(x) {\n    future(.f(x))\n  })\n  map_dbl(fs, value)\n}\n\n\nBy now, you probably have one map() and one map_dbl() inside your function. It is helpful to point out that it is the map_dbl() one that makes parallel_map_dbl() emulate what purrr::map_dbl() does. The other map() is just used to create our futures and put them in a list. We could equally well use lapply() for that. We could even use a for loop as we used in Lab 1. Because of this, all of the following alternative solutions work equally well:\n\n\nSolution 1\n\n\nparallel_map_dbl &lt;- function(.x, .f) {\n  fs &lt;- purrr::map(.x, function(x) {\n    future(.f(x))\n  })\n  purrr::map_dbl(fs, value)\n}\n\n\n\n\nSolution 2\n\n\nparallel_map_dbl &lt;- function(.x, .f) {\n  fs &lt;- lapply(.x, function(x) {\n    future(.f(x))\n  })\n  purrr::map_dbl(fs, value)\n}\n\n\n\n\nSolution 3\n\n\nparallel_map_dbl &lt;- function(.x, .f) {\n  fs &lt;- list()\n  for (ii in seq_along(X)) {\n    x &lt;- .x[[ii]]\n    fs[[ii]] &lt;- future(.f(x))\n  }\n  purrr::map_dbl(fs, value)\n}\n\n\n\n\nThings that are problematic\nTask 3:\nRun the following:\n\nxs &lt;- list(1:25, 26:50, 51:75, 76:100)\nys &lt;- list()\npurrr::map(seq_along(xs), function(ii) {\n  ys[[ii]] &lt;- slow_sum(xs[[ii]])\n})\nys\n\nWhy doesn’t it work?\nTask 4:\nDo you think the following can be parallelized?\n\nys &lt;- list(0)  # initialize with zero\nfor (ii in 2:length(xs)) {\n  x &lt;- xs[[ii]]\n  y &lt;- ys[[ii - 1]]\n  ys[[ii]] &lt;- slow_sum(x + y)\n}\n\n\n\n\n\n\n\n\nPause here!\n\n\n\nLet’s pause here! Please let the tutor know when you got here.\n\n\n\n\n\nErrors and parallel processing\nThe Futureverse has been designed such that your experience running parallel code will be as close as possible to when you run regular, sequential code. For example, if we call:\n\nx &lt;- \"1.2\"\ny &lt;- log(x)\n\nError in log(x): non-numeric argument to mathematical function\n\n\nwe get an error.\nTask 5:\nTry the with a future() call and a value() call. Start by calling:\n\nf &lt;- future(log(x))\n\nDid you get an error or not? What could be the reason for that?\n\nNext, ask for the value of the future;\n\ny &lt;- value(f)\n\nWhat happens?\n\nTask 6:\nAsk for the value one more time;\n\ny &lt;- value(f)\n\nWhat happens now? What if you keep calling value(f) over and over?\nTask 7:\nIf we use purrr as in:\n\nlibrary(purrr)\n\nxs &lt;- list(\"1.2\", 42, 3.14)\ny &lt;- map_dbl(xs, log)\n\nError in `map_dbl()`:\nℹ In index: 1.\nCaused by error:\n! non-numeric argument to mathematical function\n\n\nwe get an error, because the first element of the xs list holds a string instead of a numeric value. That is what the error message tries to explain to us.\nLet’s try with furrr and future_map_dbl() function from above.\n\nlibrary(furrr)\nplan(multisession, workers = 4)\n\nxs &lt;- list(\"1.2\", 42, 3.14)\ny &lt;- future_map_dbl(xs, log)\n\nDoes it behave as you expected? Do you notice anything different? If so, let’s talk about it.\n\n\n\n\n\n\nNote\n\n\n\nAt first, it might appear obvious that we should get an error in these cases and that it will look the same as when running regular sequential code. But rest assured, Futureverse is the only parallel framework that behave this way. If you use one of the traditional frameworks you will get a different type of error, or not an error at all. This is the case for parLapply() and mclapply() of parallel. \n\n\nTask 8:\nNext, try the same but with mclapply() of the parallel package;\n\nlibrary(parallel)\n\nxs &lt;- list(\"1.2\", 42)\nys &lt;- mclapply(xs, log)\nprint(ys)\n\nWhat happened - did you get an error? With the behavior you observed, would you be able figure out what is wrong? Also, what is the risk with the current behavior?\nTask 9:\nNext, try the same but with parLapply() of the parallel package;\n\nlibrary(parallel)\nworkers &lt;- makeCluster(4)\n\nxs &lt;- list(\"1.2\", 42)\nys &lt;- parLapply(xs, log, cl = workers)\nprint(ys)\n\nstopCluster(workers)\n\nWhat happened - did you get an error? With the behavior you observed, would you be able figure out what is wrong?\n\n\nWarnings and parallel processing\nJust like errors, warnings are signalled as-is when parallelizing via futures.\nTask 10:\n\nlibrary(furrr)\nplan(multisession, workers = 4)\n\nxs &lt;- list(42, -1.2, 3.14)\nys &lt;- future_map(xs, log)\nys\n\nDid you get a warning?\nTask 11:\nTry the same with mclapply();\n\nlibrary(parallel)\nxs &lt;- list(42, -1.2, 3.14)\nys &lt;- mclapply(xs, log)\nys\n\nDid you get a warning?\nThen, try with parLapply();\n\nlibrary(parallel)\nworkers &lt;- makeCluster(4)\nxs &lt;- list(42, -1.2, 3.14)\nys &lt;- parLapply(xs, log, cl = workers)\nys\nstopCluster(workers)\n\nDid you get a warning?\n\n\n\n\n\n\nNote\n\n\n\nFutureverse is the only parallel framework that relays errors, warnings, messages, and output from parallel workers wherever they run in the world back to your R console.\n\n\n\n\n\n\n\n\n\nPause here!\n\n\n\nLet’s pause here! Please let the tutor know when you got here.\n\n\n\n\n\nProgress updates\nYou can generate progress updates using the progressr package.\nTask 12:\nCreate the following:\n\nlibrary(progressr)\n\nslow_sum &lt;- function(x) {\n  p &lt;- progressor(along = x)  ## create progressor of length(x)\n  \n  sum &lt;- 0\n  for (value in x) {\n    p()                       ## signal progress\n    Sys.sleep(1.0)\n    sum &lt;- sum + value\n  }\n  \n  sum\n}\n\nThen call:\n\ny &lt;- slow_sum(1:5)\n\nWhat happened?\nTask 13:\nNothing happened, because we never told progressr we, as end-users, are interested in the progress updates. To do that, we need to “subscribe” to the progress events, which we can do by calling:\n\nprogressr::handlers(global = TRUE)\n\nonce at the top of our R script.\nAfter this, retry with:\n\ny &lt;- slow_sum(1:5)\n\nTask 14:\nIf you run R from RStudio, the default progress bar is reported using the built-in RStudio progress bar. If you run R from the terminal or in VSCode, the default progress report uses an old-fashioned progress bar that is built-in to R. We could tweak it to be a little bit more colorful:\n\nprogressr::handlers(\n  progressr::handler_txtprogressbar(char = cli::col_red(cli::symbol$heart))\n)\n\nand call\n\ny &lt;- slow_sum(1:5)\n\nTask 15:\nThere are other ways to report on progress too. The cli package generates colorful, nice looking progress bars in the terminal. Try with:\n\nprogressr::handlers(\"cli\")\n\nTask 16:\nLet’s try to re-customize the default cli progress bar, e.g.\n\nprogressr::handlers(\n  progressr::handler_cli(format = \"{cli::pb_spin} {cli::pb_bar} {cli::pb_current}/{cli::pb_total} {cli::pb_status}\")\n)\n\nand call\n\ny &lt;- slow_sum(1:5)\n\nTask 17:\nThus far we have done progress reporting when running sequentially, but progressr works also when running in parallel using Futureverse.\nLet’s start by creating a utility function:\n\nslow_sum_all &lt;- function(xs) {\n  p &lt;- progressr::progressor(along = xs)\n  y &lt;- furrr::future_map_dbl(xs, function(x) {\n    sum &lt;- slow_sum(x)\n    p()\n    sum\n  })\n  sum(y)\n}\n\nthat we can use as:\n\nxs &lt;- list(1:10, 11:40, 41:60, 61:100)\ny &lt;- slow_sum_all(xs)\ny\n\nNow, run it in parallel with two parallel workers. Pay attention to processing time and progress bar.\nTask 18:\nRetry with four parallel workers. Then go back to sequential processing."
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "Lab: Futureverse 1",
    "section": "",
    "text": "Note\n\n\n\nThis is the first of two parallelization labs. It will take you through some basic steps to parallelize your code using Futureverse. It focuses on core functions future() and value() for the purpose of illustrating what happens behind the scenes when we parallelize R code.\nYou are highly encouraged to test things out yourself and tweak things to figure out how these methods behave.\nSlides: You’ll find the slides in the menus above."
  },
  {
    "objectID": "lab1.html#install",
    "href": "lab1.html#install",
    "title": "Lab: Futureverse 1",
    "section": "Install",
    "text": "Install\nWe will start out by installing common Futureverse packages part of the Futureverse. We will not need them all in this lab, but it is convenient to have them all installed already now.\n\ninstall.packages(\"futureverse\")"
  },
  {
    "objectID": "lab1.html#exercises",
    "href": "lab1.html#exercises",
    "title": "Lab: Futureverse 1",
    "section": "Exercises",
    "text": "Exercises\nIn order to illustrate parallelization, we need two things: (i) a way to measure time, and (ii) something that takes at least a few seconds to run.\nTask 1:\nCopy and paste the following two code blocks.\nCreate functions tic() and toc() to measure time:\n\ntic &lt;- function() {\n  tic_start &lt;&lt;- base::Sys.time()\n}\n\ntoc &lt;- function() {\n  dt &lt;- base::difftime(base::Sys.time(), tic_start)\n  dt &lt;- round(dt, digits = 1L)\n  message(paste(format(dt), \"since tic()\"))\n}\n\nThese functions can be used as a timer, e.g.\n\ntic()\nSys.sleep(1.5)\ntoc()\n\n\n\n1.5 secs since tic()\n\n\n\nSys.sleep(4.0)\ntoc()\n\n\n\n5.5 secs since tic()\n\n\nNext, create toy function slow_sum() for calculating the sum of a vector really slowly:\n\nslow_sum &lt;- function(x) {\n  sum &lt;- 0\n  for (value in x) {\n    Sys.sleep(1.0)     ## one-second slowdown per value\n    sum &lt;- sum + value\n  }\n  sum\n}\n\nThis function works just like sum(), but it is very slow. If we use it to calculate \\(1 + 2 + \\ldots + 10\\), it will takes us ten seconds to get the result;\n\ntic()\ny &lt;- slow_sum(1:10)\ny\ntoc()\n\nMake sure you can run the latter, that it takes ten seconds to complete and that it returns the correct value.\nWe are now ready to get rolling!\n\nSimple parallel tasks\nAt the very core of Futureverse is the future package. Let us start out by loading this core package:\n\nlibrary(future)\n\nIt provides us with the fundamental building blocks for running R code in parallel; functions future(), value(), and resolved(). Other Futureverse packages, such as future.apply, furrr, and doFuture, rely on these three functions to build up more feature-rich functions. We will return to those later, but for now we will focus on future() and value().\nTask 2:\nLet’s start by writing our initial example using futures:\n\ntic()\nf &lt;- future(slow_sum(1:10))\ny &lt;- value(f)\ntoc()\n\nConfirm that you get the correct result. Did it run faster?\n\nTask 3:\nAdd another toc() just after the future() call;\n\ntic()\nf &lt;- future(slow_sum(1:10))\ntoc()\ny &lt;- value(f)\ntoc()\ny\ntoc()\n\nHow long did the creation of the future take?\n\nTask 4:\nBy design, Futureverse runs everything sequentially by default. We can configure it run code in parallel using two background workers as:\n\nplan(multisession, workers = 2)\n\nMake this change, and rerun the above example. Did the different steps take as long as you expected? What do you think the reason is for the change?\n\nTask 5:\nLet’s calculate \\(1 + 2 + \\ldots + 10\\) in two steps: (a) \\(1 + 2 +\n\\ldots + 5\\) and (b) \\(6 + 7 + \\ldots + 10\\), and then sum the two results.\n\nfa &lt;- future(slow_sum(1:5))\nfb &lt;- future(slow_sum(6:10))\ny &lt;- value(fa) + value(fb)\ny\n\nBut first, make sure to add toc() after each statement to better understand how long each step takes;\n\ntic()\nfa &lt;- future(slow_sum(1:5))\ntoc()\nfb &lt;- future(slow_sum(6:10))\ntoc()\ny &lt;- value(fa) + value(fb)\ntoc()\ny\ntoc()\n\nMake sure you get the expected result. Did it finish sooner? Which step takes the longest? Why do you think that is?\n\n\n\nCreate many parallel tasks via a for loop\nTask 6:\nHere is a very complicated way of calculating the sum \\(1 + 2 + \\ldots\n+ 20\\) in four chunks and outputting messages to show the progress:\n\ntic()\nxs &lt;- list(1:5, 6:10, 11:15, 16:20)\nys &lt;- list()\nfor (ii in seq_along(xs)) {\n  message(paste0(\"Iteration \", ii))\n  ys[[ii]] &lt;- slow_sum(xs[[ii]])\n}\nmessage(\"Done\")\nprint(ys)\n\nys &lt;- unlist(ys)\nys\n\ny &lt;- sum(ys)\ny\ntoc()\n\nRewrite it such that each iteration is parallelized via a future. Use four parallel workers as in:\n\nlibrary(future)\nplan(multisession, workers = 4)\n\n\nTask 7:\nRetry with three parallel workers as in:\n\nlibrary(future)\nplan(multisession, workers = 3)\n\nDid you notice something? What do you think happened?\n\n\nOur own parallel lapply\nTask 8:\nAbove, you used a for-loop to parallelize tasks. See if you can achieve the same using lapply() instead.\nTask 9:\nTake your parallel lapply() code and wrap it up in a function parallel_lapply() that takes two arguments X and FUN so that we can call:\n\nlibrary(future)\nplan(multisession)\n\nxs &lt;- list(1:5, 6:10, 11:15, 16:20)\n\nys &lt;- parallel_lapply(xs, slow_sum)\nys &lt;- unlist(ys)\ny &lt;- sum(ys)\n\n\n\nSolution\n\nparallel_lapply &lt;- function(X, FUN) {\n  ## Create futures that calls FUN(X[[1]]), FUN(X[[2]]), ...\n  fs &lt;- lapply(X, function(x) {\n    ## For element 'x', create future that calls FUN(x)\n    future(FUN(x))\n  })\n  \n  ## Collect the values from all futures\n  value(fs)\n}"
  },
  {
    "objectID": "lab/hausdorff_dist.html",
    "href": "lab/hausdorff_dist.html",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "",
    "text": "We have simulated 3D functional data for this lab that is provided in the Quarto document in the dat object.\nThe dat object is a list of size \\(100\\) containing \\(100\\) three-dimensional curves observed on a common grid of size \\(200\\) of the interval \\([0, 1]\\).\nAs a result, each element of the dat list is a \\(3 \\times 200\\) matrix.\nThe data looks like this:\n\nplot(mfdat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjective\n\n\n\nThe goal is to implement a function similar to stats::dist() which computes the pairwise distance matrix on this functional dataset using the Hausdorff distance."
  },
  {
    "objectID": "lab/hausdorff_dist.html#data-goal",
    "href": "lab/hausdorff_dist.html#data-goal",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "",
    "text": "We have simulated 3D functional data for this lab that is provided in the Quarto document in the dat object.\nThe dat object is a list of size \\(100\\) containing \\(100\\) three-dimensional curves observed on a common grid of size \\(200\\) of the interval \\([0, 1]\\).\nAs a result, each element of the dat list is a \\(3 \\times 200\\) matrix.\nThe data looks like this:\n\nplot(mfdat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjective\n\n\n\nThe goal is to implement a function similar to stats::dist() which computes the pairwise distance matrix on this functional dataset using the Hausdorff distance."
  },
  {
    "objectID": "lab/hausdorff_dist.html#basic-r-sequential-version",
    "href": "lab/hausdorff_dist.html#basic-r-sequential-version",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "Basic R sequential version",
    "text": "Basic R sequential version\n\nHausdorff distance\nWe can implement the Hausdorff distance between two curves as:\n\nhausdorff_distance &lt;- function(x, y) {\n  dX &lt;- max(purrr::map_dbl(x, \\(.x) {\n    min(sqrt(colSums((y - .x)^2)))\n  }))\n  dY &lt;- max(purrr::map_dbl(y, \\(.y) {\n    min(sqrt(colSums((x - .y)^2)))\n  }))\n  max(dX, dY)\n}\n\nhausdorff_distance(dat[[1]], dat[[2]])\n\n[1] 6.505233\n\n\n\n\nA first solution\n\n\n\n\n\n\ndist objects\n\n\n\nTake a look at the documention of the stats::dist() function to understand how to make an object of class dist.\n\n\n\ndist_hausdorff &lt;- function(x) {\n  N &lt;- length(x)\n  out &lt;- unlist(purrr::map(1:(N - 1), \\(.i) {\n    purrr::map_dbl((.i + 1):N, \\(.j) {\n      hausdorff_distance(x[[.i]], x[[.j]])\n    })\n  }))\n  attributes(out) &lt;- NULL\n  attr(out, \"Size\") &lt;- N\n  lbls &lt;- rownames(x)\n  attr(out, \"Labels\") &lt;- if (is.null(lbls)) 1:N else lbls\n  attr(out, \"Diag\") &lt;- FALSE\n  attr(out, \"Upper\") &lt;- FALSE\n  attr(out, \"call\") &lt;- rlang::call_match()\n  attr(out, \"method\") &lt;- \"dist_l2\"\n  class(out) &lt;- \"dist\"\n  out\n}\n\nsystem.time(D &lt;- dist_hausdorff(dat))\n\n   user  system elapsed \n 53.080   1.193  54.275"
  },
  {
    "objectID": "lab/hausdorff_dist.html#your-turn",
    "href": "lab/hausdorff_dist.html#your-turn",
    "title": "Hausdorff Distance Matrix Computation",
    "section": "Your turn",
    "text": "Your turn\n\nOptimize the R code;\nParallelize the optimized R code with futureverse.\n\n\n\n\n\n\n\nFrom linear index to matrix index\n\n\n\n\nm &lt;- nrow(A)\nr &lt;- ((ind - 1) %% m) + 1\nc &lt;- floor((ind - 1) / m) + 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Instructions for the tutorial",
    "section": "",
    "text": "This repository is build on the work of Henrik Bengtsson, who created the {future} package and the {futureverse} suite of packages. It is a fork of Henrik’s repository here with hopefully some modifications to make it more suitable for the ANF tutorial."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Instructions for the tutorial",
    "section": "",
    "text": "This repository is build on the work of Henrik Bengtsson, who created the {future} package and the {futureverse} suite of packages. It is a fork of Henrik’s repository here with hopefully some modifications to make it more suitable for the ANF tutorial."
  },
  {
    "objectID": "index.html#material",
    "href": "index.html#material",
    "title": "Instructions for the tutorial",
    "section": "Material",
    "text": "Material\nMaterial for the Futureverse tutorial at the ANF High Performance Computing with R, Frejus, France.\nTotal time: 3 hours (tentatively).\nThe main webpage is at https://astamm.github.io/anf-rhpc-futureverse/.\n\nRequirements\n\nR: https://www.r-project.org\nRStudio: https://posit.co/download/rstudio-desktop/\nQuarto: https://quarto.org/docs/get-started/\nR packages:\n\n{futureverse}\n{purrr}\n{rlang}\n{roahd}\n{tictoc}"
  },
  {
    "objectID": "futureverse-faq.html",
    "href": "futureverse-faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Benchmarking\n\n\n\nAll time benchmarks are done using the {tictoc} package and on a MacBook Pro 2021 with an Apple M1 Pro chip including 10 cores and 32 GB of RAM under Sonoma 14.5 macOS.\n\n\n\n\nLet us create a slow log function:\n\nslow_log &lt;- function(x) {\n  Sys.sleep(1)\n  log(x)\n}\n\nWe can apply this function to the integers from 1 to 10 for example using purrr::map_dbl():\n\nx1 &lt;- 1:10\n\n\ntic()\npurrr::map_dbl(x1, slow_log)\ntoc()\n\n  [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n  [8] 2.0794415 2.1972246 2.3025851\n\n10.064 sec elapsed\nAs expected, it takes 10 seconds to run.\nWe can parallelize this computation using furrr::future_map_dbl():\n\nplan(multisession, workers = 2)\ntic()\nfurrr::future_map_dbl(x1, slow_log)\ntoc()\n\n  [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n  [8] 2.0794415 2.1972246 2.3025851\n\n5.256 sec elapsed\nAs expected, it takes 5 seconds to run.\nNow, assume that the last input value has wrongly be stored as character. Then the previous run will fail with an error. Let us see this in action:\n\nx2 &lt;- c(as.list(1:9), \"10\")\ntic()\nfurrr::future_map_dbl(x2, slow_log)\ntoc()\n\nError:\nℹ In index: 5.\nCaused by error in `log()`:\n! non-numeric argument to mathematical function\n\n5.231 sec elapsed\n\n\n\n\n\n\nApparent problem\n\n\n\nWe typically parallelize long time-consuming computations. In the context of map-reduce operations, we can have a situation where one of the tasks fails possibly after a large number of computations that succeeded. This can happen for example if the input data is not as expected. In this case, the error is caught and relayed to the user but the computation stops and elements on which computation succeeded are not returned. This is not ideal as we would like (i) to continue the computation on the other tasks and (ii) to have a way to retrieve the results of the tasks that succeeded and even of the computations that succeeded on the failed task.\n\n\n\n\n\n\n\n\n\n\n\nDesign strategy\n\n\n\nThe goal of the future framework in the context of map-reduce operations, is to help with paralellizing the application of a long-running function to a large number of inputs.\nIf the long-running function fails on some inputs, the future framework chooses by design to behave consistently with the function that is applied. This means that if the function fails on some inputs, the future framework will stop the computation and return an error.\nThe reason for this design choice is that the future framework is responsible for the parallelization of the function application, but not for the function itself. The function is responsible for its own error handling. This design choice is consistent with the principle of separation of concerns.\n\n\nIndeed, the problem is already here with purrr::map_dbl():\n\ntic()\npurrr::map_dbl(x2, slow_log)\ntoc()\n\nError in `purrr::map_dbl()`:\nℹ In index: 10.\nCaused by error in `log()`:\n! non-numeric argument to mathematical function\n\n10.107 sec elapsed\nThe error is caught, the computation stops and no result is returned. One may think that this is due to the purrr::map_dbl() function and might try to use lapply() instead:\n\ntic()\nlapply(x2, slow_log)\ntoc()\n\nError in log(x): non-numeric argument to mathematical function\n\n10.048 sec elapsed\nBut the result is the same. The error is caught, the computation stops and no result is returned.\n\n\n\n\n\n\nTrue problem\n\n\n\nThe underlying problem is that, in the context of applying the long-running slow_log() function repeatedly, we are not happy with the default behavior of stopping the computation when an error occurs in the base::log() function.\n\n\n\n\n\nThe encouraged solution is therefore to handle the error in the function itself and to return a sentinel value when an error occurs. This way, the computation can continue until the end and we can retrieve the results of the tasks that succeeded.\nWe can for example implement a new version of the slow_log() function that returns NA when an error occurs and issues a warning instead of an error:\n\nslow_log2 &lt;- function(x) {\n  tryCatch({\n    slow_log(x)\n  }, error = function(e) {\n    warning(conditionMessage(e))\n    NA_real_\n  })\n}\n\nWe can then parallelize this computation using furrr::future_map_dbl():\n\nplan(multisession, workers = 2)\ntic()\nfurrr::future_map_dbl(x2, slow_log2)\ntoc()\n\nWarning in value[[3L]](cond): non-numeric argument to mathematical function\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246        NA\n\n5.279 sec elapsed\nAs expected, it takes 5 seconds to run. Futhermore, it does not stop when an error occurs and returns NA for the failed computation which allows for the computation to continue until the end and retrieve the results of the tasks that succeeded.\nHowever, one could argue that the warning message is not informative enough because it does not give an indication of which input caused the error. Ideally one would like to know the index of the input that caused the error in the original input list of values. This requires to handle the error as a warning at the level of the future_map_dbl() function and therefore one can define a future_map_log() function which may look like this:\n\nfuture_map_log &lt;- function(.x, ..., \n                           .options = furrr::furrr_options(), \n                           .env_globals = parent.frame(), \n                           .progress = FALSE) {\n  furrr::future_imap_dbl(.x, \\(x, y) {\n    tryCatch({\n      slow_log(x)\n    }, error = function(e) {\n      cli::cli_alert_warning(\"Non-numeric input at index {y}\")\n      NA_real_\n    })\n  }, .options = .options, .env_globals = .env_globals, .progress = .progress)\n}\n\nThis leads to:\n\nplan(multisession, workers = 2)\ntic()\nfuture_map_log(x2)\ntoc()\n\n! Non-numeric input at index 10\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246        NA\n\n5.301 sec elapsed\nYou can find more details in Henrik Bengsston’s vignette dedicated to common issues with futures."
  },
  {
    "objectID": "futureverse-faq.html#error-handling",
    "href": "futureverse-faq.html#error-handling",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Benchmarking\n\n\n\nAll time benchmarks are done using the {tictoc} package and on a MacBook Pro 2021 with an Apple M1 Pro chip including 10 cores and 32 GB of RAM under Sonoma 14.5 macOS.\n\n\n\n\nLet us create a slow log function:\n\nslow_log &lt;- function(x) {\n  Sys.sleep(1)\n  log(x)\n}\n\nWe can apply this function to the integers from 1 to 10 for example using purrr::map_dbl():\n\nx1 &lt;- 1:10\n\n\ntic()\npurrr::map_dbl(x1, slow_log)\ntoc()\n\n  [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n  [8] 2.0794415 2.1972246 2.3025851\n\n10.064 sec elapsed\nAs expected, it takes 10 seconds to run.\nWe can parallelize this computation using furrr::future_map_dbl():\n\nplan(multisession, workers = 2)\ntic()\nfurrr::future_map_dbl(x1, slow_log)\ntoc()\n\n  [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n  [8] 2.0794415 2.1972246 2.3025851\n\n5.256 sec elapsed\nAs expected, it takes 5 seconds to run.\nNow, assume that the last input value has wrongly be stored as character. Then the previous run will fail with an error. Let us see this in action:\n\nx2 &lt;- c(as.list(1:9), \"10\")\ntic()\nfurrr::future_map_dbl(x2, slow_log)\ntoc()\n\nError:\nℹ In index: 5.\nCaused by error in `log()`:\n! non-numeric argument to mathematical function\n\n5.231 sec elapsed\n\n\n\n\n\n\nApparent problem\n\n\n\nWe typically parallelize long time-consuming computations. In the context of map-reduce operations, we can have a situation where one of the tasks fails possibly after a large number of computations that succeeded. This can happen for example if the input data is not as expected. In this case, the error is caught and relayed to the user but the computation stops and elements on which computation succeeded are not returned. This is not ideal as we would like (i) to continue the computation on the other tasks and (ii) to have a way to retrieve the results of the tasks that succeeded and even of the computations that succeeded on the failed task.\n\n\n\n\n\n\n\n\n\n\n\nDesign strategy\n\n\n\nThe goal of the future framework in the context of map-reduce operations, is to help with paralellizing the application of a long-running function to a large number of inputs.\nIf the long-running function fails on some inputs, the future framework chooses by design to behave consistently with the function that is applied. This means that if the function fails on some inputs, the future framework will stop the computation and return an error.\nThe reason for this design choice is that the future framework is responsible for the parallelization of the function application, but not for the function itself. The function is responsible for its own error handling. This design choice is consistent with the principle of separation of concerns.\n\n\nIndeed, the problem is already here with purrr::map_dbl():\n\ntic()\npurrr::map_dbl(x2, slow_log)\ntoc()\n\nError in `purrr::map_dbl()`:\nℹ In index: 10.\nCaused by error in `log()`:\n! non-numeric argument to mathematical function\n\n10.107 sec elapsed\nThe error is caught, the computation stops and no result is returned. One may think that this is due to the purrr::map_dbl() function and might try to use lapply() instead:\n\ntic()\nlapply(x2, slow_log)\ntoc()\n\nError in log(x): non-numeric argument to mathematical function\n\n10.048 sec elapsed\nBut the result is the same. The error is caught, the computation stops and no result is returned.\n\n\n\n\n\n\nTrue problem\n\n\n\nThe underlying problem is that, in the context of applying the long-running slow_log() function repeatedly, we are not happy with the default behavior of stopping the computation when an error occurs in the base::log() function.\n\n\n\n\n\nThe encouraged solution is therefore to handle the error in the function itself and to return a sentinel value when an error occurs. This way, the computation can continue until the end and we can retrieve the results of the tasks that succeeded.\nWe can for example implement a new version of the slow_log() function that returns NA when an error occurs and issues a warning instead of an error:\n\nslow_log2 &lt;- function(x) {\n  tryCatch({\n    slow_log(x)\n  }, error = function(e) {\n    warning(conditionMessage(e))\n    NA_real_\n  })\n}\n\nWe can then parallelize this computation using furrr::future_map_dbl():\n\nplan(multisession, workers = 2)\ntic()\nfurrr::future_map_dbl(x2, slow_log2)\ntoc()\n\nWarning in value[[3L]](cond): non-numeric argument to mathematical function\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246        NA\n\n5.279 sec elapsed\nAs expected, it takes 5 seconds to run. Futhermore, it does not stop when an error occurs and returns NA for the failed computation which allows for the computation to continue until the end and retrieve the results of the tasks that succeeded.\nHowever, one could argue that the warning message is not informative enough because it does not give an indication of which input caused the error. Ideally one would like to know the index of the input that caused the error in the original input list of values. This requires to handle the error as a warning at the level of the future_map_dbl() function and therefore one can define a future_map_log() function which may look like this:\n\nfuture_map_log &lt;- function(.x, ..., \n                           .options = furrr::furrr_options(), \n                           .env_globals = parent.frame(), \n                           .progress = FALSE) {\n  furrr::future_imap_dbl(.x, \\(x, y) {\n    tryCatch({\n      slow_log(x)\n    }, error = function(e) {\n      cli::cli_alert_warning(\"Non-numeric input at index {y}\")\n      NA_real_\n    })\n  }, .options = .options, .env_globals = .env_globals, .progress = .progress)\n}\n\nThis leads to:\n\nplan(multisession, workers = 2)\ntic()\nfuture_map_log(x2)\ntoc()\n\n! Non-numeric input at index 10\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246        NA\n\n5.301 sec elapsed\nYou can find more details in Henrik Bengsston’s vignette dedicated to common issues with futures."
  },
  {
    "objectID": "futureverse-faq.html#wrong-index-information-upon-error-in-furrr",
    "href": "futureverse-faq.html#wrong-index-information-upon-error-in-furrr",
    "title": "Frequently Asked Questions",
    "section": "Wrong index information upon error in {furrr}",
    "text": "Wrong index information upon error in {furrr}\nWhen using the {purrr} package to manipulate lists instead of base R *apply() family, the user gets to know the index of the element that caused an error:\n\npurrr::map_dbl(list(1, 2, 3, \"4\"), log)\n\nError in `purrr::map_dbl()`:\nℹ In index: 4.\nCaused by error:\n! non-numeric argument to mathematical function\n\n\nParallel versions of the purrr::*map*() functions are provided by the {furrr} package. We would expect the same behavior when an error occurs in a future:\n\nplan(multisession, workers = 2)\nfurrr::future_map_dbl(list(1, 2, 3, \"4\"), log)\n\nError:\nℹ In index: 2.\nCaused by error:\n! non-numeric argument to mathematical function\n\n\nThis is not the case. The error message does not provide the correct index of the element that caused the error. What it currently does is to provide the index in the subset of the data that was sent to the worker that failed. This is not very helpful as the user is interested in the index in the original data. This is a known issue and is discussed in Issue #250 on the GitHub repository of the package.\nIn the meantime, a workaround is to use the idea behind the purrr::imap_* functions which keeps track of the index of the element in the original list:\n\nmy_future_map_dbl &lt;- function(.x, .f, ..., \n                              .options = furrr::furrr_options(), \n                              .env_globals = parent.frame(), \n                              .progress = FALSE) {\n  furrr::future_imap_dbl(.x, \\(x, y) {\n    tryCatch({\n      .f(x)\n    }, error = function(e) {\n      cli::cli_alert_danger(\"Non-numeric input at index {y} in the original input list.\", wrap = TRUE)\n      cli::cli_alert_warning(\"The index mentioned in the message below refers to the subset of the data that was sent to the worker that failed.\", wrap = TRUE)\n    })\n  }, .options = .options, .env_globals = .env_globals, .progress = .progress)\n}\n\nThis leads to:\n\nplan(multisession, workers = 2)\nmy_future_map_dbl(list(1, 2, 3, \"4\"), log)\n\n✖ Non-numeric input at index 4 in the original input list.\n\n\n! The index mentioned in the message below refers to the subset of the data\nthat was sent to the worker that failed.\n\n\nError:\nℹ In index: 2.\nCaused by error:\n! Can't coerce from a string to a double."
  },
  {
    "objectID": "futureverse-faq.html#futureverse-optimised-blas-mkl-openblas-veclib",
    "href": "futureverse-faq.html#futureverse-optimised-blas-mkl-openblas-veclib",
    "title": "Frequently Asked Questions",
    "section": "Futureverse + optimised BLAS (MKL, OpenBLAS, vecLib)",
    "text": "Futureverse + optimised BLAS (MKL, OpenBLAS, vecLib)\n\n\n\n\n\n\nBenchmarking\n\n\n\nAll time benchmarks are done using the {tictoc} package and on a Dell computer with Intel(R) Xeon(R) W-10885M CPU @ 2.40GHz processor including 8 cores and 64 GB of RAM under Windows 11.\n\n\n\nContext\nIt is possible to boost matrix calculations in R by replacing the default BLAS library with an optimized one. The most popular optimized BLAS libraries are Intel’s Math Kernel Library (MKL), OpenBLAS and vecLib (on macOS). See these articles for how to setup and/or some benchmarking:\n\nhttps://thomasmcrow.com/blog/2021-08-optimized-blas-in-r/\nhttps://mpopov.com/blog/2019/06/04/faster-matrix-math-in-r-on-macos/\nhttps://csantill.github.io/RPerformanceWBLAS/\nhttps://stateofther.github.io/finistR2023/Intel_MKL.html\n\nThese optimized BLAS libraries are typically faster than the default BLAS library that comes with R by exploiting vectorization and parallelization.\n\n\nProblem setup\n\n\n\n\n\n\nProblem\n\n\n\nThe problem is that combining the optimized BLAS libraries with the future framework seems to deteriorate the performance of the optimized BLAS and leads to increased computation time.\n\n\n\n\nUse of parallelization in optimized BLAS libraries\nBy default, it uses all available cores. However, the optimization is mostly due to vectorization and rather than parallelization. See this webpage for a better understanding of the gain one can expect from MKL. Importantly, it is mentioned that:\n\nMost of the benefit of the Intel MKL is from vectorised math, not multi-threading. A big performance boost when using the MKL with just one thread. A marginal increase when using 4 threads, most notable in matrix multiply, and no benefit for singular value decomposition.\n\nAnother resource on HCP here.\n\n\nExample 1: matrix inversion\n\n## Have a look at the nb of cores / threads\n## Nb cores\nRhpcBLASctl::get_num_cores()\n## Nb threads \nRhpcBLASctl::get_num_procs()\n\n## Generate a list of matrices to inverse\nN &lt;- 16    ## Matrix number\np &lt;- 2000   ## Matrix size\nx &lt;- lapply(1:N, \\(i) matrix(rnorm(p * p), p, p) + diag(p))\n\n## Make a function that controls the number of threads used by the math lib \n## to compute the matrix inverse\ncompute_inv &lt;- function(x, nbthr = 1) {\n  RhpcBLASctl::blas_set_num_threads(threads = nbthr)\n  solve(x)\n}\n\n\n## Default \ntic()\nres &lt;- lapply(x, solve)\ntoc()\n\n2.56 sec elapsed\n\n## Then disable MKL parallel computation\ntic()\nres &lt;- lapply(x, compute_inv, nbthr = 1)\ntoc()\n\n2.71 sec elapsed\n\n## Then use future but without parallelization\nplan(sequential)\ntic()\nres &lt;- furrr::future_map(x, compute_inv, nbthr = 1)\ntoc()\n\n2.8 sec elapsed\nReducing the nb of threads does not affect the computational time! The computational cost reduction is due to vectorization, not multi-threading.\n\n## Now use 2 workers\nplan(multisession, workers = 2)\ntic()\nres &lt;- furrr::future_map(x, compute_inv, nbthr = 1)\ntoc()\n\n4.19 sec elapsed\n\n## Now use 4 workers\nplan(multisession, workers = 4)\ntic()\nres &lt;- furrr::future_map(x, compute_inv, nbthr = 1)\ntoc()\n\n5.03 sec elapsed\n\n## Now use 8 workers\nplan(multisession, workers = 8)\ntic()\nres &lt;- furrr::future_map(x, compute_inv, nbthr = 1)\ntoc()\n\n5.5 sec elapsed\n\n## Now use 16 workers\nplan(multisession, workers = 16)\ntic()\nres &lt;- furrr::future_map(x, compute_inv, nbthr = 1)\ntoc()\n\n7.68 sec elapsed\nSystematic downgrading of the performance, the higher the nb of workers, the higher the computational time.\n\n\nExample 2: matrix product\n\n## Generate a list of pairs of matrices to multiply with a ref matrix\nN &lt;- 16    ## Matrix number\nNbRow &lt;- 5000   ## Matrix nb row\nNbCol &lt;- 1000   ## Matrix nb row\n\nx &lt;- lapply(1:N, \\(i) matrix(rnorm(NbRow * NbCol), NbRow, NbCol))\nRefMat &lt;- matrix(rnorm(NbRow * NbCol), NbRow, NbCol) \n\n## Make a function that controls the number of threads used by the math lib \n## to compute the matrix crossprod\nComputeProd &lt;- function(x,nbthr=1){\n  blas_set_num_threads(threads = nbthr)\n  crossprod(x,RefMat)\n}\n\nUsing the same setup as for Example 1, we get the following computational times:\n\nDefault (MKL parallel computation): 0.67 sec;\nDisable MKL parallel computation: 2.74 sec;\nFuture without parallelization: 2.95 sec;\nFuture with 2 workers: 2.95 sec;\nFuture with 4 workers: 2.89 sec;\nFuture with 8 workers: 4 sec;\nFuture with 16 workers: 6.71 sec.\n\n\n\nCurrent understanding\nFor now the results are not intuitive at all:\n\nsolve mostly makes use of vectorization from MKL, as illustrated by the fact that changing the number of threads does not change the performance;\nin contrast, crossprod benefits from multi-threading: reducing the number of threads downgrades the computational speed.\n\nSo we expect that reducing the number of threads to \\(1\\) will not hamper solve(), allowing the use of futures for parallelization over matrices. We should observe some (significant ?) gain. However, as soon as we increase the number of workers, we downgrade the performance. Differently, in the matrix product case, if we reduce the number of threads used by MKL, we allow for parallelization over matrices via futures but at the cost of some increase in computational time for each cross product. So there seems to be a trade-off, possibly hard to deal with, between the implicit multi-threading of MKL and the explicit one of future. However we basically observe no impact when the number of workers is low, and a downgrade when it is increased…"
  },
  {
    "objectID": "futureverse-faq.html#using-source-in-a-future",
    "href": "futureverse-faq.html#using-source-in-a-future",
    "title": "Frequently Asked Questions",
    "section": "Using source() in a future",
    "text": "Using source() in a future\nAccording to Henrik Bengsston’s vignette dedicated to common issues with futures:\n\nAvoid using source() inside futures. It is always better to source external R scripts at the top of your main R script, e.g.\n\n\nlibrary(future)\nsource(\"./my-script.R\")\n\nf &lt;- future({\n  ...\n})\n\n\nHowever, if you find yourself having to source a script inside a future, or inside a function, make sure to specify argument local = TRUE, e.g.\n\n\nf &lt;- future({\n  source(\"./my-script.R\", local = TRUE)\n  ...\n})\n\n\nThis is because source() defaults to local = FALSE, which has side effects. When using local = FALSE, any functions or variables defined by the R script are assigned to the global environment - not the calling environment as we might expect. This may make little different when calling source() from the R prompt, or from another script. However, when called from inside a function, inside local(), or inside a future, it might result in unexpected behavior. It is similar to using assign(\"a\", 42, envir = globalenv()), which is known be a bad practice. To be on the safe side, it is almost always better call source() with local = TRUE."
  },
  {
    "objectID": "futureverse-faq.html#sharing-big-matrices-between-r-processes",
    "href": "futureverse-faq.html#sharing-big-matrices-between-r-processes",
    "title": "Frequently Asked Questions",
    "section": "Sharing big matrices between R processes",
    "text": "Sharing big matrices between R processes\n\nThe {bigmemory} package provides mechanisms for working with very large matrices that can be updated in-place, which helps save memory.\n\n\n\n\n\n\n\nProblem of non-exportable objects\n\n\n\n\nSome types of R objects can be used only in the R session they were created. If used as-is in another R process, such objects often result in an immediate error or in obscure and hard-to-troubleshoot outcomes. Because of this, they cannot be saved to file and re-used at a later time. They may also not be exported to a parallel worker when doing parallel processing. These objects are sometimes referred to as non-exportable or non-serializable objects.\n\n\n\nObjects of class big.matrix are non-exportable as-is. This means that they cannot be exported to a parallel worker when doing parallel processing. This is because the object is a reference to a memory-mapped file, and the worker does not have access to the memory-mapped file.\n\n\n\n\n\n\nThe marshalling solution\n\n\n\n\nOne solution to this problem is to use “marshalling” to encode the R object into an exportable representation that then can be used to re-create a copy of that object in another R process that imitates the original object.\n\n\n\n\nThe {marshal} package provides generic functions marshal() and unmarshal() for marshalling and unmarshalling R objects of certain class. This makes it possible to save otherwise non-exportable objects to file and then be used in a future R session, or to transfer them to another R process to be used there.\n\nAs part of the development of the marshal package, the author has listed a number of classes that are non-exportable and assessed whether they can be marshalled and whether they must be marshalled. The list is available in an article from the package website here. One can see that the class big.matrix is in the list of non-exportable classes and has been assessed as a class that can and must be marshalled. This means that the marshal package will soon be able to marshal and unmarshal objects of class big.matrix.\n\n\n\n\n\n\nActive working group on marshalling and serialization\n\n\n\nMay 2024: The R Consortium ISC Working Group ‘Marshaling and Serialization in R’ has been launched to work on this problem. So we can expect some progress in the near future."
  },
  {
    "objectID": "futureverse-faq.html#nested-parallelization-with-future",
    "href": "futureverse-faq.html#nested-parallelization-with-future",
    "title": "Frequently Asked Questions",
    "section": "Nested parallelization with {future}",
    "text": "Nested parallelization with {future}\n\n\n\n\n\n\nBenchmarking\n\n\n\nAll time benchmarks are done using the {tictoc} package and on a Lenovo LOQ 15IRH8 with an Intel i5-12450H including 12 cores and 16 GB of RAM under Ubuntu 24.04.1 LTS.\n\n\n\nlibrary(future)\nlibrary(future.apply)\nlibrary(future.callr)\nlibrary(progressr)\n\n\nNested task definition\nWe define the following task where we split a sum in sub-sums and perform this multiple times. To slow down computations, we use progressr::slow_sum.\n\ndelay &lt;- 0.5\n\nlist_sums &lt;- list(1:10, 11:20)\nlist_list_sums &lt;- rep(list(list_sums), 2L)\nfuture_nested_loop &lt;- function() {\n    unlist(future.apply::future_lapply(list_list_sums, function(l_sums) {\n        v_sums &lt;- unlist(\n            future.apply::future_lapply(l_sums, slow_sum,\n                delay = delay,\n                message = FALSE\n            )\n        )\n        sum(v_sums)\n    }))\n}\n\nBelow we implement a checking function to ensure everything went fine and the returned result was correct.\n\nexpected_result &lt;- rep(sum(unlist(list_sums)), length(list_list_sums))\n\ncheck_result &lt;- function(task_time, task_result) {\n    if (!all(task_result == expected_result)) {\n        cli::cli_abort(message = c(\"{task_time$msg} result do not match expected result\",\n            \"x\" = \"Expected : {expected_result} got {task_result}\"\n        ))\n    } else {\n        cli::cli_alert_success(\"Task returned expected result\")\n    }\n}\n\n\n\nR and base plan with {future}\n\nBase R and plan(sequential)\nWith R base functions :\n\ntic(\"Base R\")\nbase_r_result &lt;- unlist(lapply(list_list_sums, function(l_sums) {\n    v_sums &lt;- unlist(\n        lapply(l_sums, slow_sum, delay = delay, message = FALSE)\n    )\n    sum(v_sums)\n}))\nbase_r_time &lt;- toc()\n\nBase R: 20.076 sec elapsed\n\ncheck_result(task_time = base_r_time, base_r_result)\n\n✔ Task returned expected result\n\n\nWith our delay values we have the following :\n\n\\(10 \\times 0.5s\\) for each sub-sum.\n\\(2 \\times 10 \\times 0.5s\\) for each sub-list of sub-sum\n\nSo \\(2 \\times 2 \\times 10 \\times 0.5 s\\) for the list of sub-lists of sub-sums, so \\(20 s\\). Which is what we approximately observe with {tictoc}:\n\\(20.076 s\\).\nAvec le plan sequential\n\nplan(sequential)\ntic(\"Future plan(sequential)\")\nsequential_result &lt;- future_nested_loop()\nsequential_time &lt;- toc()\n\nFuture plan(sequential): 20.145 sec elapsed\n\ncheck_result(\n    task_time = sequential_time,\n    task_result = sequential_result\n)\n\nWith sequential plan it takes \\(20.145\\) s, which is comparable withR base functions.\n\n\nplan(multisession)\n\nplan(tweak(\"multisession\", workers = 2L))\ntic(\"Future plan(tweak('multisession', workers = 2L))\")\nmultisession2_result &lt;- future_nested_loop()\nmultisession2_time &lt;- toc()\n\nFuture plan(tweak('multisession', workers = 2L)): 10.365 sec elapsed\n\ncheck_result(\n    task_time = multisession2_time,\n    task_result = multisession2_result\n)\n\n✔ Task returned expected result\n\n\nBy allocating 2 workers with multisession we manage to obtain a duration below sequential: \\(10.365\\) s.\n\nplan(multisession(workers = 4L))\ntic(\"Future plan(multisession(workers = 4L))\")\nmultisession4_result &lt;- future_nested_loop()\nmultisession4_time &lt;- toc()\n\nFuture plan(multisession(workers = 4L)): 10.404 sec elapsed\n\ncheck_result(\n    task_time = multisession4_time,\n    task_result = multisession4_result\n)\n\n✔ Task returned expected result\n\n\nBy allocating 4 workers, we observe no improvement, it takes: \\(10.404\\) s.\nThis is because {future} doesn’t leverage nested parallelization by default, the first future_lapply being on a two elements list, the maximal reduction time is obtained with two workers. The other two allocated in this example are not used.\n\n\n\nNested parallelization\nAccording to the documentation we can specify not only a single plan but a list of plans, that can apply to nested futures.\n\nplan(list(multisession, multisession))\ntic(\"Future plan(list(multisession, multisession))\")\nlist_multisession_result &lt;- future_nested_loop()\nlist_multisession_time &lt;- toc()\n\nFuture plan(list(multisession, multisession)): 10.651 sec elapsed\n\ncheck_result(\n    task_time = list_multisession_time,\n    task_result = list_multisession_result\n)\n\n✔ Task returned expected result\n\n\nBut at the opposite of what we expected, we do not lower the time taken but it increases ! The task takes: \\(10.651\\) s.\n\n\n\n\n\n\nNote\n\n\n\nAs Henrik Bengtsson explains in the vignette Future Topologies, {future} has a protection against recursive parallelism. And thus the second loop is not parallelized but forced to be sequential.\nThe plan must thus be specified in a specific manner to work as intended.\n\n\n\nPlan lists with explicit parameters\nGiven the structure of our task we want the first loop to run on the 2 elements of the list in parallel, thus we need 2 workers for the first future_lapply.\nEach sub-list containing 2 elements, we want to allocate each 2 workers.\nAs we want to parallelize the whole task we need: \\(2 \\times 2 =\n4\\) workers to allocate.\n\nplan(list(tweak(multisession, workers = 2L), tweak(multisession, workers = 2L)))\ntic(\"Future plan(list(tweak(multisession, workers = 2L), tweak(multisession, workers = 2L)))\")\nlist_multisession2_2_result &lt;- rlang::try_fetch(\n    future_nested_loop(),\n    error = function(cnd) inform(\"Task failed.\", parent = cnd)\n)\n\nWarning in checkNumberOfLocalWorkers(workers): Careful, you are setting up 2\nlocalhost parallel workers with only 1 CPU cores available for this R process\n(per ‘mc.cores’), which could result in a 200% load. The soft limit is set to\n100%. Overusing the CPUs has negative impact on the current R process, but also\non all other processes of yours and others running on the same machine. See\nhelp(\"parallelly.options\", package = \"parallelly\") for how to override the soft\nand hard limits\nWarning in checkNumberOfLocalWorkers(workers): Careful, you are setting up 2\nlocalhost parallel workers with only 1 CPU cores available for this R process\n(per ‘mc.cores’), which could result in a 200% load. The soft limit is set to\n100%. Overusing the CPUs has negative impact on the current R process, but also\non all other processes of yours and others running on the same machine. See\nhelp(\"parallelly.options\", package = \"parallelly\") for how to override the soft\nand hard limits\n\nlist_multisession2_2_time &lt;- toc()\n\nFuture plan(list(tweak(multisession, workers = 2L), tweak(multisession, workers = 2L))): 6.349 sec elapsed\n\ncheck_result(\n    task_time = list_multisession2_2_time,\n    task_result = list_multisession2_2_result\n)\n\n✔ Task returned expected result\n\n\nWith this plan we get some warnings, explaining the we are over-parallelizing. We get this message even if we have the correct number of cores (or workers) to properly run this task.\nThis is {future} defense mecanism against recursive parallelism.\n\n\n\n\n\n\nWhy shouldn’t I ignore this warning\n\n\n\n{future} has a soft limit (that gives warnings) and a hard limit, when reaching the hard limit (of 300% load per core) the future won’t run, even if there is enough available cores.\nThe next callout block and next code block explains how to tell {future} that this plan is ok.\n\n\n\n\n\n\n\n\nLet {future} know that we want recursive parallelism\n\n\n\nTo tell the package that we know what we are doing, we must use I() which gives AsIs class to the object it wraps.\n\n\n\nplan(list(tweak(multisession, workers = 2L), tweak(multisession, workers = I(2L))))\ntic(\"Future plan(list(tweak(multisession, workers = 2L), tweak(multisession, workers = I(2L))))\")\nlist_multisession2_I2_result &lt;- rlang::try_fetch(\n    future_nested_loop(),\n    error = function(cnd) inform(\"Task failed.\", parent = cnd)\n)\nlist_multisession2_I2_time &lt;- toc()\n\nFuture plan(list(tweak(multisession, workers = 2L), tweak(multisession, workers = I(2L)))): 6.396 sec elapsed\n\ncheck_result(\n    task_time = list_multisession2_I2_time,\n    task_result = list_multisession2_I2_result\n)\n\n✔ Task returned expected result\n\n\nAnd this way we can specify a working nested parallelization plan.\n\n\nWith {future.callr}\nThe {future.callr} package solves some limitations of future::multisession() and do not need the use of I() as shown in the below example:\n\nplan(list(tweak(callr, workers = 2L), tweak(callr, workers = 2L)))\ntic(\"Future `plan(list(tweak(callr, workers = 2L), tweak(callr, workers = 2L)))`\")\nlist_callr2_2_result &lt;- rlang::try_fetch(\n    future_nested_loop(),\n    error = function(cnd) inform(\"Task failed.\", parent = cnd)\n)\nlist_callr2_2_time &lt;- toc()\n\nFuture `plan(list(tweak(callr, workers = 2L), tweak(callr, workers = 2L)))`: 6.299 sec elapsed\n\ncheck_result(\n    task_time = list_callr2_2_time,\n    task_result = list_callr2_2_result\n)\n\n✔ Task returned expected result\n\n\n\n\n\nExecution time summary table\n\n\n\n\n\n\n\n\n\nfuture::plan(.)\nElapsed time\n\n\n\n\nBase R\n20.076\n\n\nsequential\n20.145\n\n\nmultisession(workers = 2L)\n10.365\n\n\nmultisession(workers = 4L)\n10.404\n\n\nlist(multisession, multisession)\n10.651\n\n\nlist(tweak(multisession, workers = 2L), tweak(multisession, workers = 2L))\n6.349\n\n\nlist(tweak(multisession, workers = 2L), tweak(multisession, workers = I(2L)))\n6.396\n\n\nlist(tweak(callr, workers = 2L), tweak(callr, workers = 2L))\n6.299"
  }
]